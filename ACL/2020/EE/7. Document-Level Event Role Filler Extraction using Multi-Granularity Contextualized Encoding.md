# Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding
## Abstract
Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.
> 关于事件提取的文献中，只有很少的著作超越了单个句子来做出提取决策。当识别事件自变量所需的信息分布在多个句子中时，这是有问题的。我们认为，文档级事件提取是一项艰巨的任务，因为它需要一个更大的上下文视图来确定哪些文本范围与事件角色填充符相对应。我们首先研究端到端神经序列模型（具有预先训练的语言模型表示）在文档级角色填充符提取中的性能，以及捕获的上下文长度如何影响模型的性能。为了动态汇总在不同粒度级别（例如句子和段落级别）学习的神经表示所捕获的信息，我们提出了一种新颖的多粒度阅读器。我们在MUC-4事件提取数据集上评估了我们的模型，并表明我们最好的系统比以前的工作表现更好。我们还报告了有关任务的上下文长度和神经模型性能之间关系的发现。
## Keywords
- event extraction
- document-level event extraction
- neural sequence models
## Key Information
- **Authors:** Xinya Du, Claire Cardie
- **Organizations**: Department of Computer Science, Cornell University
- **Datasets**: [MUC-4](datasets/muc34.tar.gz)
- **Codes**: <https://github.com/xinyadu/doc_event_role>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.714/), [Pdf](https://github.com/Clearailhc/KG-NLP-Papers/blob/main/ACL/2020/EE/pdf/2020.acl-main.714.pdf), [Video](http://slideslive.com/38928745), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.714.bib)
