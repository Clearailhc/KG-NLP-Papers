# Improving Event Detection via Open-domain Trigger Knowledge
## Abstract
Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.
## Keywords
- event detection
- open-domain trigger knowledge
## Key Information
- **Authors:** Meihan Tong; Bin Xu; Shuai Wang; Yixin Cao; Lei Hou; Juanzi Li; Jun Xie
- **Organizations**: Knowledge Engineering Laboratory, Tsinghua University; SLP Group, AI Technology Department, JOYY Inc; National University of Singapore; SPPD Group, Tencent Inc
- **Datasets**: ACE 2005
- **Codes**: <https://github.com/shuaiwa16/ekd.git>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.522/), [Pdf](https://github.com/Clearailhc/KG-NLP-Papers/blob/main/ACL/2020/EE/pdf/2020.acl-main.522.pdf), [Video](http://slideslive.com/38928727>), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.522.bib)
