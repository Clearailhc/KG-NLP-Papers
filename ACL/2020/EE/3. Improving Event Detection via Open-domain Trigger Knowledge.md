# Improving Event Detection via Open-domain Trigger Knowledge
## Abstract
Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.
> 事件检测（ED）是自动构造文本的一项基本任务。 由于训练数据的规模小，以前的方法在看不见/标记稀疏的触发词上效果较差，并且容易过度拟合密集标记的触发词。 为了解决该问题，我们提出了一种新颖的扩展知识蒸馏（EKD）模型，以利用外部开放域触发知识来减少注释中频繁触发词的内置偏差。 在基准ACE2005上进行的实验表明，我们的模型优于9个强基准，对于看不见/稀疏标记的触发词特别有效。 源代码在https://github.com/shuaiwa16/ekd.git上发布。
## Keywords
- event detection
- open-domain trigger knowledge
## Key Information
- **Authors:** Meihan Tong; Bin Xu; Shuai Wang; Yixin Cao; Lei Hou; Juanzi Li; Jun Xie
- **Organizations**: Knowledge Engineering Laboratory, Tsinghua University; SLP Group, AI Technology Department, JOYY Inc; National University of Singapore; SPPD Group, Tencent Inc
- **Datasets**: ACE 2005
- **Codes**: <https://github.com/shuaiwa16/ekd.git>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.522/), [Pdf](https://github.com/Clearailhc/KG-NLP-Papers/blob/main/ACL/2020/EE/pdf/2020.acl-main.522.pdf), [Video](http://slideslive.com/38928727), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.522.bib)
