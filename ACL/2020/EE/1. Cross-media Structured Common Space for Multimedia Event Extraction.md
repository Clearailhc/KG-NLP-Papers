# Cross-media Structured Common Space for Multimedia Event Extraction
## Abstract
We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.
## Keywords
- Event Extraction
- Multimedia
## Key Information
- **Authors:** Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang
- **Organizations**: University of Illinois at Urbana-Champaign, Columbia University
- **Datasets**: [M<sup>2</sup>E<sup>2</sup> dataset](https://github.com/limanling/m2e2)
- **Codes**: <http://blender.cs.illinois.edu/software/m2e2>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.230/), [Pdf](https://github.com/Clearailhc/KG-NLP-Papers/blob/main/ACL/2020/EE/pdf/2020.acl-main.230.pdf), [Video](http://slideslive.com/38928686>), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.230.bib)


