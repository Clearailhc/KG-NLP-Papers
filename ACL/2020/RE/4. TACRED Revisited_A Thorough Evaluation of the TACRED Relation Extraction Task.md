#  TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task
## Abstract
TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.
> 文档级关系提取需要在文档的多个句子之内和之间整合信息，并捕获句子间实体之间的复杂交互。但是，有效汇总文件中的相关信息仍然是一个具有挑战性的研究问题。现有方法基于来自非结构化文本的语法树，共同引用或启发式方法来构造静态文档级图，以对依赖关系进行建模。与以前的方法可能无法捕获丰富的非本地交互作用来进行推理不同，我们提出了一种新颖的模型，该模型通过自动引入潜在的文档级图来增强句子之间的关系推理。我们进一步开发了一种细化策略，该策略使模型能够逐步汇总相关信息以进行多跳推理。具体来说，我们的模型在大型文档级数据集（DocRED）上获得了59.05的F1分数，与以前的结果相比有了显着改善，并且在CDR和GDA数据集上也产生了最新的最新结果。此外，大量分析表明，该模型能够发现更准确的句子间关系。
## Keywords
- Document-level relation extraction
- latent document-level graph
- multi-hop reasoning
## Key Information
- **Authors:** Guoshun Nan; Zhijiang Guo; Ivan Sekulic; Wei Lu
- **Organizations**: StatNLP Research Group, Singapore University of Technology and Design; Università della Svizzera italiana
- **Datasets**:  [DocRED](https://github.com/thunlp/DocRED)
- **Codes**: <https://github.com/nanguoshun/LSR>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.141/), [Pdf](pdf/2020.acl-main.141.pdf), [Video](http://slideslive.com/38929374), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.141.bib)


