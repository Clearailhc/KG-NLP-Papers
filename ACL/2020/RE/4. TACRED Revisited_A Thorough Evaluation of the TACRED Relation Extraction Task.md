#  TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task
## Abstract
TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.
> TACRED是关系提取（RE）中最大，使用最广泛的众包数据集之一。但是，即使在无监督预训练和知识增强型神经RE的最新进展中，模型仍然显示出较高的错误率。在本文中，我们调查以下问题：我们是否达到了性能极限或仍有改进的空间？人群注释，数据集和模型如何导致此错误率？为了回答这些问题，我们首先使用训练有素的注释器来验证开发和测试集中最具挑战性的5K示例。我们发现标签错误占F1测试绝对错误的8％，并且有超过50％的示例需要重新标签。在重新标记的测试集上，大型基线模型集的平均F1分数从62.1提高到70.1。验证之后，我们将对具有挑战性的实例进行错误分类分析，将其归类为语言错误，并在三个最新的RE模型上验证错误假设。我们表明两组歧义关系是造成大多数剩余错误的原因，并且当实体未被掩盖时，模型可能会在数据集上采用浅试探法。
## Keywords
- Relation Extraction
- TACRED dataset
## Key Information
- **Authors:** Christoph Alt; Aleksandra Gabryszak; Leonhard Hennig
- **Organizations**: German Research Center for Artificial Intelligence (DFKI), Speech and Language Technology Lab
- **Datasets**:  [DocRED](https://github.com/thunlp/DocRED)
- **Codes**: <https://github.com/nanguoshun/LSR>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.142/), [Pdf](pdf/2020.acl-main.142.pdf), [Video](http://slideslive.com/38929374), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.142.bib)


