#  Reasoning with Latent Structure Refinement for Document-Level Relation Extraction
## Abstract
Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.
> 文档级关系提取需要在文档的多个句子之内和之间整合信息，并捕获句子间实体之间的复杂交互。但是，有效汇总文件中的相关信息仍然是一个具有挑战性的研究问题。现有方法基于来自非结构化文本的语法树，共同引用或启发式方法来构造静态文档级图，以对依赖关系进行建模。与以前的方法可能无法捕获丰富的非本地交互作用来进行推理不同，我们提出了一种新颖的模型，该模型通过自动引入潜在的文档级图来增强句子之间的关系推理。我们进一步开发了一种细化策略，该策略使模型能够逐步汇总相关信息以进行多跳推理。具体来说，我们的模型在大型文档级数据集（DocRED）上获得了59.05的F1分数，与以前的结果相比有了显着改善，并且在CDR和GDA数据集上也产生了最新的最新结果。此外，大量分析表明，该模型能够发现更准确的句子间关系。
## Keywords
- Document-level relation extraction
- latent document-level graph
- multi-hop reasoning
## Key Information
- **Authors:** Guoshun Nan; Zhijiang Guo; Ivan Sekulic; Wei Lu
- **Organizations**: StatNLP Research Group, Singapore University of Technology and Design; Università della Svizzera italiana
- **Datasets**:  [DocRED](https://github.com/thunlp/DocRED)
- **Codes**: <https://github.com/nanguoshun/LSR>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.141/), [Pdf](pdf/2020.acl-main.141.pdf), [Video](http://slideslive.com/38929374), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.141.bib)


