# Finding Universal Grammatical Relations in Multilingual BERT
## Abstract
Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.
> 最近的工作发现，基于变压器的多语言掩蔽语言模型Multilingual BERT（mBERT）能够零射击跨语言传输，这表明其表示的某些方面是跨语言共享的。 为了更好地理解这种重叠，我们将最近在神经网络的内部表示中查找语法树的工作扩展到了多语言环境。 我们表明，mBERT表示的子空间可以用英语以外的其他语言恢复语法树距离，并且这些子空间在各种语言之间是近似共享的。 受这些结果的启发，我们提出了一种无监督的分析方法，该方法可提供证据证明mBERT学习了句法依赖性标签的表示形式，其形式为与通用依赖性分类法基本相符的聚类。 该证据表明，即使没有明确的监督，多语言掩蔽语言模型也会学习某些语言通用性。
## Keywords
- zero-shot
- Multilingual
## Key Information
- **Authors:** Ethan A. Chi; John Hewitt; Christopher D. Manning
- **Organizations**: Department of Computer Science, Stanford University
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.acl-main.493/), [Pdf](pdf/2020.acl-main.493.pdf), [Video](http://slideslive.com/38929418), [BibTex](https://www.aclweb.org/anthology/2020.acl-main.493.bib)


