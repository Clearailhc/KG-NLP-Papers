# New Benchmark Corpus and Models for Fine-grained Event Classification: To BERT or not to BERT?
## Abstract
We introduce a new set of benchmark datasets derived from ACLED data for fine-grained event classification and compare the performance of various state-of-the-art models on these datasets, including SVM based on TF-IDF character n-grams and neural context-free embeddings (GLOVE and FASTTEXT) as well as deep learning-based BERT with its contextual embeddings. The best results in terms of micro (94.3-94.9%) and macro F1 (86.0-88.9%) were obtained using BERT transformer, with simpler TF-IDF character n-gram based SVM being an interesting alternative. Further, we discuss the pros and cons of the considered benchmark models in terms of their robustness and the dependence of the classification performance on the size of training data.
> 我们引入了一组来自ACLED数据的基准测试数据集，用于细粒度事件分类，并比较了这些数据集上各种最新模型的性能，包括基于TF-IDF字符n-gram和神经环境的SVM 嵌入（GLOVE和FASTTEXT），以及基于上下文学习的基于深度学习的BERT。 使用BERT变压器可获得在微型（94.3-94.9％）和宏F1（86.0-88.9％）方面的最佳结果，其中更简单的基于TF-IDF字符n-gram的SVM是有趣的选择。 此外，我们从基准模型的健壮性和分类性能对训练数据大小的依赖性方面讨论了基准模型的优缺点。
## Keywords
- fine-grained event classification
- benchmark datasets
## Key Information
- **Authors:** Jakub Piskorski; Jacek Haneczok; Guillaume Jacquet
- **Organizations**: Institute for Computer Science, Polish Academy of Sciences, Warsaw, Poland; Erste Group IT, Vienna, Austria; Joint Research Centre, European Commission, Isrpa, Italy

- **Datasets**: [Armed Conflict Location & Event Data Project (ACLED)](https://www.acleddata.com/curated-data-files/)
- **Codes**: None
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.coling-main.584/), [Pdf](pdf/2020.coling-main.584.pdf), [BibTex](https://www.aclweb.org/anthology/2020.coling-main.584.bib)
