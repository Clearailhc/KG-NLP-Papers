# Incremental Event Detection via Knowledge Consolidation Networks
## Abstract
Conventional approaches to event detection usually require a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC-KBP benchmark, respectively.

> 传统的事件检测方法通常需要一组固定的预定义事件类型。随着新事件的不断发生，这种需求在现实世界的应用程序中经常受到挑战。由于巨大的计算成本和存储预算，每当新事件到来时，存储所有以前的数据并用所有以前的数据和新数据重新训练模型是不可行的。我们将诸如增量事件检测之类的挑战性场景表述为必需的模型，该模型需要一个模型来逐步学习新的类，而不会降低先前类的性能。但是，现有的增量学习方法无法处理语义歧义性，并且无法在增量事件检测任务中训练新旧类之间的数据不平衡问题。在本文中，我们提出了一个知识整合网络（KCN）来解决上述问题。具体来说，我们设计了两个组件，即原型增强追溯和分层蒸馏，以分别减轻语义歧义和类不平衡的不利影响。实验结果证明了该方法的有效性，在ACE基准和TAC KBP基准上，它们分别比最新模型高出了整个F1分数的19％和13.4％
## Keywords
- event detection
- Knowledge Consolidation Network
- knowledge increment
- model distillation
## Key Information
- **Authors:** Pengfei Cao, Yubo Chen, Jun Zhao, Taifeng Wang
- **Organizations**: National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences
- **Datasets**: [ACE 2005](https://catalog.ldc.upenn.edu/LDC2006T06), [TAC-KBP](https://tac.nist.gov/2017/KBP/data.html)
- **Codes**: <https://github.com/CPF-NLPR/IncrementalED>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.emnlp-main.52/), [Pdf](https://github.com/Clearailhc/KG-NLP-Papers/blob/main/EMNLP/2020/EE/pdf/2020.emnlp-main.52.pdf), [Video], [Slides], [BibTex](https://www.aclweb.org/anthology/2020.emnlp-main.52.bib)
