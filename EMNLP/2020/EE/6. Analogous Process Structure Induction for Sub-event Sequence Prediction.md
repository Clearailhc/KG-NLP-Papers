# Analogous Process Structure Induction for Sub-event Sequence Prediction
## Abstract
Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as “buying a car” can be used in the context of a new but analogous process such as “buying a house”. Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.

> 事件理解的计算和认知研究表明，识别，理解和预测事件取决于对事件序列进行结构化表示，并将其组成部分概念化（抽象）为（软）事件类别。 因此，可以在诸如“购买房屋”之类的新的但类似的过程的上下文中使用关于诸如“购买汽车”之类的已知过程的知识。 但是，NLP中的大多数事件理解工作仍处于底层，并且不考虑抽象。 在本文中，我们提出了一个类似的过程结构归纳（APSI）框架，该框架利用过程之间的类比和子事件实例的概念化来预测以前未见过的开放域过程的整个子事件序列。 正如我们的实验和分析所表明的那样，APSI支持为看不见的过程生成有意义的子事件序列，并可以帮助预测丢失的事件。
## Keywords
- event understanding
- sub-event sequences
- Analogous Process Structure
## Key Information
- **Authors:** Hongming Zhang; Muhao Chen; Haoyu Wang; Yangqiu Song; Dan Roth
- **Organizations**: Department of Computer Science and Engineering, HKUST; Department of Computer and Information Science, UPenn

- **Datasets**: [WikiHow](https://www.wikihow.com/), [ASER](https://dl.acm.org/doi/pdf/10.1145/3366423.3380107)
- **Codes**: <https://cogcomp.seas.upenn.edu/page/publication_view/910>
- **Urls:** [ACL Page](https://www.aclweb.org/anthology/2020.emnlp-main.119/), [Pdf](https://github.com/Clearailhc/KG-NLP-Papers/blob/main/EMNLP/2020/EE/pdf/2020.emnlp-main.119.pdf), Video, Slides, [BibTex](https://www.aclweb.org/anthology/2020.emnlp-main.119.bib)
